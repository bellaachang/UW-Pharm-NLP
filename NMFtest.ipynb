{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 104 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPD_Q1</th>\n",
       "      <th>CPD_Q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The class time devoted to health and wellness ...</td>\n",
       "      <td>I believe self-care to be really important in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The class time devoted to health and wellness ...</td>\n",
       "      <td>The time spent on my choice of self-care allow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The class time has reminded me on the importan...</td>\n",
       "      <td>The time spent on my choice of self care affec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The class time that was devoted to health and ...</td>\n",
       "      <td>It made me more relaxed and less stressed abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have learnt to listen to people without inte...</td>\n",
       "      <td>This has been a lifeline because I always felt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              CPD_Q1  \\\n",
       "0  The class time devoted to health and wellness ...   \n",
       "1  The class time devoted to health and wellness ...   \n",
       "2  The class time has reminded me on the importan...   \n",
       "3  The class time that was devoted to health and ...   \n",
       "4  I have learnt to listen to people without inte...   \n",
       "\n",
       "                                              CPD_Q3  \n",
       "0  I believe self-care to be really important in ...  \n",
       "1  The time spent on my choice of self-care allow...  \n",
       "2  The time spent on my choice of self care affec...  \n",
       "3  It made me more relaxed and less stressed abou...  \n",
       "4  This has been a lifeline because I always felt...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/choiceboard_data.csv\")[[\"CPD_Q1\", \"CPD_Q3\"]]\n",
    "data = data.dropna()\n",
    "print('There are ' + str(data.shape[0]) + ' rows.')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPD_Q1</th>\n",
       "      <th>CPD_Q3</th>\n",
       "      <th>CPD_Q1 Clean</th>\n",
       "      <th>CPD_Q3 Clean</th>\n",
       "      <th>CPD_Q1 Word Count</th>\n",
       "      <th>CPD_Q3 Word Count</th>\n",
       "      <th>CPD_Q1 Clean Word Count</th>\n",
       "      <th>CPD_Q3 Clean Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The class time devoted to health and wellness ...</td>\n",
       "      <td>I believe self-care to be really important in ...</td>\n",
       "      <td>[class, time, devot, health, well, self, care,...</td>\n",
       "      <td>[believ, self, care, realli, import, live, hea...</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The class time devoted to health and wellness ...</td>\n",
       "      <td>The time spent on my choice of self-care allow...</td>\n",
       "      <td>[class, time, devot, health, well, self, care,...</td>\n",
       "      <td>[time, spent, choic, self, care, allow, relax,...</td>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The class time has reminded me on the importan...</td>\n",
       "      <td>The time spent on my choice of self care affec...</td>\n",
       "      <td>[class, time, remind, import, self, care, also...</td>\n",
       "      <td>[time, spent, choic, self, care, affect, posit...</td>\n",
       "      <td>91</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The class time that was devoted to health and ...</td>\n",
       "      <td>It made me more relaxed and less stressed abou...</td>\n",
       "      <td>[class, time, devot, health, well, self, care,...</td>\n",
       "      <td>[made, relax, le, stress, upcom, futur, exam, ...</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have learnt to listen to people without inte...</td>\n",
       "      <td>This has been a lifeline because I always felt...</td>\n",
       "      <td>[learnt, listen, peopl, without, interrupt, le...</td>\n",
       "      <td>[lifelin, alway, felt, like, wast, time, walk,...</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              CPD_Q1  \\\n",
       "0  The class time devoted to health and wellness ...   \n",
       "1  The class time devoted to health and wellness ...   \n",
       "2  The class time has reminded me on the importan...   \n",
       "3  The class time that was devoted to health and ...   \n",
       "4  I have learnt to listen to people without inte...   \n",
       "\n",
       "                                              CPD_Q3  \\\n",
       "0  I believe self-care to be really important in ...   \n",
       "1  The time spent on my choice of self-care allow...   \n",
       "2  The time spent on my choice of self care affec...   \n",
       "3  It made me more relaxed and less stressed abou...   \n",
       "4  This has been a lifeline because I always felt...   \n",
       "\n",
       "                                        CPD_Q1 Clean  \\\n",
       "0  [class, time, devot, health, well, self, care,...   \n",
       "1  [class, time, devot, health, well, self, care,...   \n",
       "2  [class, time, remind, import, self, care, also...   \n",
       "3  [class, time, devot, health, well, self, care,...   \n",
       "4  [learnt, listen, peopl, without, interrupt, le...   \n",
       "\n",
       "                                        CPD_Q3 Clean  CPD_Q1 Word Count  \\\n",
       "0  [believ, self, care, realli, import, live, hea...                 88   \n",
       "1  [time, spent, choic, self, care, allow, relax,...                 78   \n",
       "2  [time, spent, choic, self, care, affect, posit...                 91   \n",
       "3  [made, relax, le, stress, upcom, futur, exam, ...                 38   \n",
       "4  [lifelin, alway, felt, like, wast, time, walk,...                 58   \n",
       "\n",
       "   CPD_Q3 Word Count  CPD_Q1 Clean Word Count  CPD_Q3 Clean Word Count  \n",
       "0                 37                       42                       20  \n",
       "1                 26                       39                       15  \n",
       "2                 63                       39                       30  \n",
       "3                 34                       16                       13  \n",
       "4                 50                       25                       23  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower() \n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)  \n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens] \n",
    "    \n",
    "    # Return preprocessed list of tokens\n",
    "    return tokens\n",
    "\n",
    "data['CPD_Q1 Clean'] = data['CPD_Q1'].apply(preprocess)\n",
    "data['CPD_Q3 Clean'] = data['CPD_Q3'].apply(preprocess)\n",
    "\n",
    "data['CPD_Q1 Word Count'] = data['CPD_Q1'].apply(lambda x: len(x.split()))\n",
    "data['CPD_Q3 Word Count'] = data['CPD_Q3'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data['CPD_Q1 Clean Word Count'] = data['CPD_Q1 Clean'].apply(lambda x: len(x))\n",
    "data['CPD_Q3 Clean Word Count'] = data['CPD_Q3 Clean'].apply(lambda x: len(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['CPD_Q1'].tolist()\n",
    "y = data['CPD_Q1'].tolist()\n",
    "\n",
    "list_of_lists = [string.split() for string in x]\n",
    "list_of_lists2 = [string.split() for string in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "modelling_data = data[['CPD_Q1', 'CPD_Q3']]\n",
    "modelling_data.iloc[:,0]\n",
    "\n",
    "corpus = modelling_data[\"CPD_Q1\"].tolist()\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a DTM\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Extract the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize a list to hold the feature names for each response\n",
    "features_per_response = []\n",
    "\n",
    "# Iterate over each response in the DTM\n",
    "for response_idx in range(dtm.shape[0]):\n",
    "    # Find the indices of non-zero features for this response\n",
    "    non_zero_indices = dtm[response_idx].nonzero()[1]\n",
    "    # Map these indices to the actual feature names\n",
    "    response_features = [feature_names[idx] for idx in non_zero_indices]\n",
    "    # Add the feature names for this response to the list\n",
    "    features_per_response.append(response_features)\n",
    "\n",
    "# 'features_per_response' now contains the list of feature names for each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of topics: 4, Coherence Score: 0.5569449880453113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Function to train NMF models for a range of topic numbers\n",
    "def train_nmf_models(doc_term_matrix, n_topics_list):\n",
    "    nmf_models = {}\n",
    "    for n_topics in n_topics_list:\n",
    "        nmf = NMF(n_components=n_topics, random_state=42)\n",
    "        W = nmf.fit_transform(doc_term_matrix)  # Document-topic matrix\n",
    "        nmf_models[n_topics] = (nmf, W)\n",
    "    return nmf_models\n",
    "\n",
    "# Function to convert NMF topics to a format compatible with Gensim\n",
    "def nmf_topics_to_gensim_format(nmf_model, feature_names, n_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        top_feature_indices = topic.argsort()[:-n_words - 1:-1]\n",
    "        topic_words = [feature_names[i] for i in top_feature_indices]\n",
    "        topics.append(topic_words)\n",
    "    return topics\n",
    "\n",
    "# Function to compute coherence scores for NMF models\n",
    "def compute_coherence_scores(nmf_models, texts, feature_names):\n",
    "    coherence_scores = {}\n",
    "    for n_topics, (nmf_model, _) in nmf_models.items():\n",
    "        topics = nmf_topics_to_gensim_format(nmf_model, feature_names)\n",
    "        \n",
    "        # Create a Gensim dictionary\n",
    "        dictionary = Dictionary(texts)\n",
    "        # Convert texts to Gensim format corpus\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        # Compute Coherence Score using c_v measure\n",
    "        coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_scores[n_topics] = coherence_score\n",
    "    \n",
    "    return coherence_scores\n",
    "\n",
    "# Main script to find the optimal number of topics\n",
    "if __name__ == \"__main__\":\n",
    "    # Your document-term matrix and associated vocabulary\n",
    "    doc_term_matrix = corpus\n",
    "    feature_names = features_per_response\n",
    "\n",
    "    # Initialize a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer to the corpus and transform the corpus into a DTM\n",
    "    doc_term_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Extract the feature names (vocabulary) from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Range of topics to evaluate\n",
    "    n_topics_list = range(2, 21)\n",
    "\n",
    "    # Train NMF models\n",
    "    nmf_models = train_nmf_models(doc_term_matrix, n_topics_list)\n",
    "\n",
    "    # Convert your corpus to a list of lists of words for coherence score calculation\n",
    "    texts = [doc.split() for doc in corpus]\n",
    "\n",
    "    # Compute coherence scores\n",
    "    coherence_scores = compute_coherence_scores(nmf_models, texts, feature_names)\n",
    "\n",
    "    # Find the number of topics with the highest coherence score\n",
    "    optimal_n_topics = max(coherence_scores, key=coherence_scores.get)\n",
    "    print(f\"Optimal number of topics: {optimal_n_topics}, Coherence Score: {coherence_scores[optimal_n_topics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 OPTIMAL TOPICS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "modelling_data = data[['CPD_Q1', 'CPD_Q3']]\n",
    "modelling_data.iloc[:,0]\n",
    "\n",
    "corpus2 = modelling_data[\"CPD_Q3\"].tolist()\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a DTM\n",
    "dtm = vectorizer.fit_transform(corpus2)\n",
    "\n",
    "# Extract the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize a list to hold the feature names for each response\n",
    "features_per_response = []\n",
    "\n",
    "# Iterate over each response in the DTM\n",
    "for response_idx in range(dtm.shape[0]):\n",
    "    # Find the indices of non-zero features for this response\n",
    "    non_zero_indices = dtm[response_idx].nonzero()[1]\n",
    "    # Map these indices to the actual feature names\n",
    "    response_features = [feature_names[idx] for idx in non_zero_indices]\n",
    "    # Add the feature names for this response to the list\n",
    "    features_per_response.append(response_features)\n",
    "\n",
    "# 'features_per_response' now contains the list of feature names for each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/bing/miniconda3/envs/newenv/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of topics: 6, Coherence Score: 0.5252602135433088\n"
     ]
    }
   ],
   "source": [
    "# Function to train NMF models for a range of topic numbers\n",
    "def train_nmf_models(doc_term_matrix, n_topics_list):\n",
    "    nmf_models = {}\n",
    "    for n_topics in n_topics_list:\n",
    "        nmf = NMF(n_components=n_topics, random_state=42)\n",
    "        W = nmf.fit_transform(doc_term_matrix)  # Document-topic matrix\n",
    "        nmf_models[n_topics] = (nmf, W)\n",
    "    return nmf_models\n",
    "\n",
    "# Function to convert NMF topics to a format compatible with Gensim\n",
    "def nmf_topics_to_gensim_format(nmf_model, feature_names, n_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        top_feature_indices = topic.argsort()[:-n_words - 1:-1]\n",
    "        topic_words = [feature_names[i] for i in top_feature_indices]\n",
    "        topics.append(topic_words)\n",
    "    return topics\n",
    "\n",
    "# Function to compute coherence scores for NMF models\n",
    "def compute_coherence_scores(nmf_models, texts, feature_names):\n",
    "    coherence_scores = {}\n",
    "    for n_topics, (nmf_model, _) in nmf_models.items():\n",
    "        topics = nmf_topics_to_gensim_format(nmf_model, feature_names)\n",
    "        \n",
    "        # Create a Gensim dictionary\n",
    "        dictionary = Dictionary(texts)\n",
    "        # Convert texts to Gensim format corpus\n",
    "        corpus2 = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        # Compute Coherence Score using c_v measure\n",
    "        coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus2, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_scores[n_topics] = coherence_score\n",
    "    \n",
    "    return coherence_scores\n",
    "\n",
    "# Main script to find the optimal number of topics\n",
    "if __name__ == \"__main__\":\n",
    "    # Your document-term matrix and associated vocabulary\n",
    "    doc_term_matrix = corpus2\n",
    "    feature_names = features_per_response\n",
    "\n",
    "    # Initialize a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer to the corpus and transform the corpus into a DTM\n",
    "    doc_term_matrix = vectorizer.fit_transform(corpus2)\n",
    "\n",
    "    # Extract the feature names (vocabulary) from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Range of topics to evaluate\n",
    "    n_topics_list = range(2, 21)\n",
    "\n",
    "    # Train NMF models\n",
    "    nmf_models = train_nmf_models(doc_term_matrix, n_topics_list)\n",
    "\n",
    "    # Convert your corpus to a list of lists of words for coherence score calculation\n",
    "    texts = [doc.split() for doc in corpus2]\n",
    "\n",
    "    # Compute coherence scores\n",
    "    coherence_scores = compute_coherence_scores(nmf_models, texts, feature_names)\n",
    "\n",
    "    # Find the number of topics with the highest coherence score\n",
    "    optimal_n_topics = max(coherence_scores, key=coherence_scores.get)\n",
    "    print(f\"Optimal number of topics: {optimal_n_topics}, Coherence Score: {coherence_scores[optimal_n_topics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
